# Submission information
submission:
  # Submission label
  # Label is used to index submissions.
  # Generate your label in the following way to avoid
  # overlapping codes among submissions:
  # [Last name of corresponding author]_[Abbreviation of institute of the corresponding author]_task[task number]_[index number of your submission (1-4)]
  label: Jeong_SEOULTECH_task1_1

  # Submission name
  # This name will be used in the results tables when space permits
  name: Student Device-Specific Fine-tuned System

  # Submission name abbreviated
  # This abbreviated name will be used in the results table when space is tight.
  # Use maximum 10 characters.
  abbreviation: StudentDev

  # Authors of the submitted system. Mark authors in
  # the order you want them to appear in submission lists.
  # One of the authors has to be marked as corresponding author,
  # this will be listed next to the submission in the results tables.
  authors:
    # First author - UPDATE WITH YOUR INFORMATION
    - lastname: Jeong
      firstname: Seunggyu
      email: wa975@naver.com          # Contact email address
      corresponding: false                   # Mark true for one of the authors
      # Affiliation information for the author
      affiliation:
        abbreviation: SEOULTECH
        institute: Seoul National University of Science and Technology
        department: Department of Artificial Intelligence
        location: Seoul, South Korea

    # Second author - UPDATE WITH YOUR INFORMATION
    - lastname: Kim
      firstname: Seongeon
      email: sekim@seoultech.ac.kr          # Contact email address
      corresponding: true                    # Mark true for one of the authors
      # Affiliation information for the author
      affiliation:
        abbreviation: SEOULTECH
        institute: Seoul National University of Science and Technology
        department: Department of Artificial Intelligence
        location: Seoul, South Korea

# System information
system:
  # System description, meta data provided here will be used to do
  # meta analysis of the submitted system.
  # Use general level tags, when possible use the tags provided in comments.
  # If information field is not applicable to the system, use "!!null".
  
  # URL to the inference code of the system [required]
  inference_code: [YOUR_GITHUB_URL]  # UPDATE WITH YOUR REPOSITORY URL
  
  # URL to the full source code (including training) of the system [optional]
  source_code: [YOUR_GITHUB_URL]  # UPDATE WITH YOUR REPOSITORY URL
  
  description:

    # Audio input / sampling rate
    # e.g. 16kHz, 22.05kHz, 32kHz, 44.1kHz, 48.0kHz
    input_sampling_rate: 44.1kHz

    # Acoustic representation
    # one or multiple labels, e.g. MFCC, log-mel energies, spectrogram, CQT, raw waveform, ...
    acoustic_features: log-mel energies

    # Data augmentation methods
    # e.g. mixup, freq-mixstyle, dir augmentation, pitch shifting, time rolling, frequency masking, time masking, frequency warping, ...
    data_augmentation: freq-mixstyle, mixup

    # Machine learning
    # e.g., (RF-regularized) CNN, RNN, CRNN, Transformer, ...
    machine_learning_method: CNN, Transformer

    # External data usage method
    # e.g. "dataset", "embeddings", "pre-trained model", ...
    external_data_usage: pre-trained model

    # Method for handling the complexity restrictions
    # e.g. "knowledge distillation", "pruning", "precision_16", "weight quantization", "network design", ...
    complexity_management: knowledge distillation

    # System training/processing pipeline stages
    # e.g. "train teachers", "ensemble teachers", "train general student model with knowledge distillation",
    # "device-specific end-to-end fine-tuning", "quantization-aware training"
    pipeline: train general teacher model, ensemble teachers, device-specific fine-tuning 

    # Machine learning framework
    # e.g. keras/tensorflow, pytorch, ...
    framework: pytorch

    # How did you exploit available device information at inference time?
    # e.g., "per-device end-to-end fine-tuning", "device-specific adapters", "device-specific normalization", ...
    device_information: "per-device end-to-end fine-tuning"
    
    # Total number of models used at inference time
    # e.g., one general model and one model for each of A, B, C, S1, S2, S3 in baseline (= 7 models)
    num_models_at_inference: 7
    
    # Degree of parameter sharing between device-specific models
    # Options: "fully shared", "partially shared", "fully device-specific"
    model_weight_sharing: "fully device-specific"

  # System complexity
  # If complexity differs across device-specific models, report values for the most complex model.
  complexity:
    # Total model size in bytes. Calculated as [parameter count]*[bit per parameter]/8
    total_model_size: 122296  # UPDATE WITH CALCULATED VALUE

    # Total number of parameters in the most complex device-specific model
    # For other than neural networks, if parameter count information is not directly
    # available, try estimating the count as accurately as possible.
    # In case of ensemble approaches, add up parameters for all subsystems.
    # In case embeddings are used, add up parameter count of the embedding
    # extraction networks and classification network
    # Use numerical value.
    total_parameters: 61148   # UPDATE WITH CALCULATED VALUE

    # MACS - as calculated by torchinfo
    macs: 26059412  # UPDATE WITH CALCULATED VALUE

  # List of external datasets used in the submission.
  external_datasets:
    # No external datasets used
    name: PaSST
    url: https://github.com/kkoutini/PaSST
    total_audio_length: !!null

# System results
results:
  development_dataset:
    # Results on the development-test set for both the general model and the device-specific models.
    #
    # - The `general` block reports results when using a single model for all devices.
    # - The `device_specific` block reports results when using a dedicated model for each known device
    #   (e.g., A, B, C, S1â€“S6), and falling back to the general model for unknown devices.
    #
    # Providing both results allows for evaluating the benefit of device-specific adaptation.
    # Partial results are acceptable, but full reporting is highly encouraged for comparative analysis.
    
    device_specific:
    # Results using device-specific models for known devices,
    # and the general model for unknown devices.
    
      # Overall metrics - UPDATE WITH YOUR RESULTS
      overall:
        logloss: !!null   # Set to !!null if not computed
        accuracy: 57.93    # mean of class-wise accuracies

      # Class-wise metrics - UPDATE WITH YOUR RESULTS
      class_wise:
        airport:           { accuracy: 55.0, logloss: !!null }
        bus:               { accuracy: 67.17, logloss: !!null }
        metro:             { accuracy: 52.36, logloss: !!null }
        metro_station:     { accuracy: 48.59, logloss: !!null }
        park:              { accuracy: 80.34, logloss: !!null }
        public_square:     { accuracy: 46.09, logloss: !!null }
        shopping_mall:     { accuracy: 69.97, logloss: !!null }
        street_pedestrian: { accuracy: 27.10, logloss: !!null }
        street_traffic:    { accuracy: 76.73, logloss: !!null }
        tram:              { accuracy: 55.91, logloss: !!null }

      # Device-wise metrics - UPDATE WITH YOUR RESULTS
      device_wise:
        a:   { accuracy: 69.82, logloss: !!null }
        b:   { accuracy: 60.67, logloss: !!null }
        c:   { accuracy: 64.80, logloss: !!null }
        s1:  { accuracy: 53.55, logloss: !!null }
        s2:  { accuracy: 54.94, logloss: !!null }
        s3:  { accuracy: 57.82, logloss: !!null }
        s4:  { accuracy: 54.67, logloss: !!null }
        s5:  { accuracy: 55.12, logloss: !!null }
        s6:  { accuracy: 50.00, logloss: !!null }
    
    general: 
    # Results using the general model (used for unknown devices in section 'device-specific') for all devices
    
      # Overall metrics - UPDATE WITH YOUR RESULTS
      overall:
        logloss: !!null   # !!null, if you don't have the corresponding result
        accuracy: 54.61   # mean of class-wise accuracies

      # Class-wise metrics - UPDATE WITH YOUR RESULTS
      class_wise:
        airport:           { accuracy: 47.6, logloss: !!null }
        bus:               { accuracy: 61.99, logloss: !!null }
        metro:             { accuracy: 54.11, logloss: !!null }
        metro_station:     { accuracy: 50.94, logloss: !!null }
        park:              { accuracy: 83.6, logloss: !!null }
        public_square:     { accuracy: 37.68, logloss: !!null }
        shopping_mall:     { accuracy: 64.21, logloss: !!null }
        street_pedestrian: { accuracy: 19.66, logloss: !!null }
        street_traffic:    { accuracy: 73.91, logloss: !!null }
        tram:              { accuracy: 52.43, logloss: !!null }

      # Device-wise metrics - UPDATE WITH YOUR RESULTS
      device_wise:
        a:   { accuracy: 64.33, logloss: !!null }
        b:   { accuracy: 52.43, logloss: !!null }
        c:   { accuracy: 57.33, logloss: !!null }
        s1:  { accuracy: 51.39, logloss: !!null }
        s2:  { accuracy: 51.7, logloss: !!null }
        s3:  { accuracy: 54.58, logloss: !!null }
        s4:  { accuracy: 54.67, logloss: !!null }
        s5:  { accuracy: 55.12, logloss: !!null }
        s6:  { accuracy: 50.0, logloss: !!null } 